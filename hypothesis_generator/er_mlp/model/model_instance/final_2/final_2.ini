[DEFAULT]
# Word embedding will use the average of the consituent words contained in the entity to produce the entity embedding.
# If false, it will randomly initialize an embedding for each entity.
word_embedding = True

# Total number of epochs to train.
training_epochs = 300

# Size of batch. Change appropriately.
batch_size = 5100

# Number of iterations till logging is displayed.
display_step = 5

# The embedding size for the entity or word.
embedding_size = 50

# The number of features for an inner layer.
layer_size = 60

# The learning rate for the optimizer.
learning_rate = 0.001

# The number of corruptions to produce per positive sample.
corrupt_size = 100

# The regularization parameter.
lambda = 0.001

# 1 for adam, 0 for adagrad.
optimizer = 1

# 0 for tanh, 1 for sigmoid.
act_function = 0

# Number of additional inner layers to add. The activation layer for each inner layer will be relu.
add_layers = 3

# The drop out percentage for each additional layer added.
drop_out_percent = 0.5

# Are we using max margin training. This is deprecated and we must use Max margin.
max_margin_training = True

# This is deprecated, use_range is no longer included.
use_range = False

# This is deprecated, use_neg is no longer included.
use_neg = False

# The train file name.
train_file = train.txt

# Train local file name.
train_local_file = train_local.txt

# Will we be saving the model to disk?
save_model = True

# F1 threshold used for optimal threshold. False will use accuracy.
f1_for_threshold = True

# The regular expression that separates words in the entities and predicates.
separator = '#SPACE#|#COMMA#|#SEMICOLON#|\W+'

# The margin used for margin based ranking loss.
margin = 0.2

# The path to the data directory.
data_path = /home/jyoun/Jason/Research/KIDS/hypothesis_generator/data/kb/final_2
